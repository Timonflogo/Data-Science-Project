---
title: "LSTM-cross"
author: "Timon Florian Godt"
date: "5/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```


```{r}
# import libraries
pacman::p_load(ISLR, dplyr, ggplot2, tidyverse, GGally, corrplot, caret, 
               e1071, MASS, class, readxl, reshape2, keras, scales, gridExtra,
               grid, zoo, lubridate, forecast, tseries, timetk, tidyquant,
               tibbletime, forcats, glue, openair)
```

```{r}
# set working directory
setwd("~/BI-2020/Data-Science/Projects/Data-Science-Project/Data")

# import dataset 
cross <- read_csv("cross-cafe-final.csv")

# get rid of Index, Aaland, and Ziggy
cross <- cross[, -c(1,3,5)]

# isolate lockdown period
names(cross)[1] <- "date"
cross2020 <- selectByDate(
  cross,
  start = "2020-05-18",
  end = "2020-12-08 ",
)

# isolate predictor variables
cross2020 <- cross2020[, -c(3,5,8:11, 13:19)]
# round values in cross2020 to 2 decimals
cross2020 <- as.data.frame(cross2020)
cross2020[is.na(cross2020)] <- 0
summary(cross2020)
```

```{r}
# Run regression to see how well our features explain sales
fit.reg <- lm(cross_cafe ~ . -date, data = cross2020)
summary(fit.reg)

# drop pressure as it has no effect on sales
cross2020 <- cross2020[, -c(5)]
str(cross2020)

# Run regression without pressure to see how well our features explain sales
fit.reg <- lm(cross_cafe ~ . -date, data = cross2020)
summary(fit.reg)
```

```{r}
glimpse(cross2020)
```

```{r}
ggplot(cross2020, aes(x = 1:nrow(cross2020), y = cross_cafe)) + geom_line()
```

```{r}
# get plot for first 10 days of temperature
ggplot(cross2020[1:240,], aes(x = 1:240, y = cross_cafe)) + geom_line()
```

```{r}
max_lag <- 10*24
firstHour.2020<- 24*(as.Date("2020-5-18 00:00:00")-as.Date("2020-1-1 00:00:00"))

# plot 2020 data
cc.ts <- ts(cross2020$cross_cafe, start(2020,firstHour.2020), frequency = 24*365)
tsdisplay(cc.ts)
acf(cc.ts, lag.max = max_lag)
```

```{r}
# we need to preprocess the data so that the RNN can understand it. It is already numerical so we don't have to do vectorization. Each time series is on a different scale which means we will need to normalize each of them independently. Afterwards we are going to write a generator function that takes the current array of float data and yields batches of of data from the recent past, along with a target temperature in the future. Since the samples in the Dataset are highly redundant (sample N and sample N+1 will have most of their timesteps in common as there is only a 10 minute time period between them) it would be wasteful to allocate every sample. we are instead going to generate the samples on the fly using the original data. 
# lets convert our dataframe into a matrix of floating points. we are NOT going to include the DATETIME column. 
cross2020 <- data.matrix(cross2020[,-1])
# data <- as.matrix(data[,-1])
# str(cross2020)
# str(data)
```

```{r}
# to normalize the data we are going to subtract the mean and divide by the standard deviation. We are going to carry out these operations for every time series we have in our matrix. 
# We are going to use the first 200.000 datapoints as our train set and therefore apply our normalization on this subset first.
train_data <- cross2020[1:2952,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
cross2020 <- scale(cross2020, center = mean, scale = std)
```

```{r}
# build the generator to yield a list (samples, targets). where samples is one batch of input data and targets is the corresponding array of target temperatures. it takes the following arguments: 
# data - the origin of floating-point data, which we normalized in the previous step
# lookback - How many timesteps back the input data should go
# delay - How many timesteps into the future the target should be
# min_index and max_index - indices in the data array that delimit which timesteps to draw from. this is useful for keeping a segment of the data for validation and another for testing
# shuffle - Whether to shuffle the samples or draw them in chronological order.
# batch_size - The number of samlpes per batch
# step - The period, in timesteps, at which you sample data. You'll set it to 6 in order to get a data point every hour. 

generator <- function(data, lookback, delay, min_index, max_index, shuffle = FALSE, batch_size = 128, step = 1) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else { 
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
    
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]],
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay, 2] 
    }
    
    list(samples, targets)
  }
}
```
I am not entirely sure how this function works but will try to explain later.

```{r}
# lets use the generator function to generate our train, validation, and test datasets
lookback <- 240 # the amount of timesteps back the input data should go
step <- 1 # the amount of steps in one hour. setting this to 6 will yield us with 1 data point every hour. 
delay <- 168
batch_size <- 128
train_gen <- generator(
  cross2020, 
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 2952,
  shuffle = TRUE,
  step = step,
  batch_size = batch_size
)
val_gen <- generator(
  cross2020,
  lookback = lookback,
  delay = delay,
  min_index = 2953,
  max_index = 3936,
  step = step,
  batch_size = batch_size
)
test_gen <- generator(
  cross2020,
  lookback = lookback,
  delay = delay,
  min_index = 3937,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)
# this is how many steps to draw from 'val_gen'
# in order to see the whole validation set:
val_steps <- (3936 - 2952 - lookback) / batch_size
# this is how many steps to draw from test_gen
test_steps <- (nrow(cross2020) - 3937 - lookback) / batch_size
```

```{r}
# create benchmark algorithm for evaluation of LSTM performance and usefulness. 
# in this prediction we are going to assume that the temperature in 24 hours is the same as right now. 
# We are going to use the Mean Absolute Error (MAE) to evalute this naive approach
evaluate_naive_method <- function() {
  batch_maes <- c()
  for (step in 1:val_steps) {
    c(samples, targets) %<-% val_gen()
    preds <- samples[,dim(samples)[[2]],2]
    mae <- mean(abs(preds - targets))
    batch_maes <- c(batch_maes, mae)
  }
  print(mean(batch_maes))
}

evaluate_naive_method()
```

```{r}
# converting the MAE back to sales error
celsius_mae <- 1.651 * std[[1]]
celsius_mae
```

```{r}
# First we are going to use a densly connected neural network as another benchmark algorithm to see whether more complex models such as LSTM is legitimate and delivers real benefits.
model <- keras_model_sequential() %>%
  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>%
  layer_dense(units=32, activation = "relu") %>%
  layer_dense(units=1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)
  
```

```{r}
# A first recurrent Baseline model which in our case will be the Gated Recurrent Unit (GRU)
model <- keras_model_sequential() %>%
  layer_gru(units=32, input_shape = list(NULL, dim(data)[[-1]])) %>%
  layer_dense(units=1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen, 
  steps_per_epoch = 500,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)
plot(history)
```

```{r}
# using recurrent dropout to fight overfitting
# add dropout and recurrent_dropout to the recurrent layer of our model
model <- keras_model_sequential() %>%
  layer_gru(units=32, dropout=0.2, recurrent_dropout = 0.2,
            input_shape = list(NULL, dim(data)[[-1]])) %>%
  layer_dense(units=1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen, 
  steps_per_epoch = 500,
  epochs = 40,
  validation_data = val_gen,
  validation_steps = val_steps
)
plot(history)
```

```{r}
# LSTM model 
lstm <- keras_model_sequential() %>%
  layer_lstm(units=32, input_shape = list(NULL, dim(data)[[-1]])) %>%
  layer_lstm(units=32, input_shape = list(NULL, dim(data)[[-1]])) %>%
  layer_dense(units=1)

lstm %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- lstm %>% fit_generator(
  train_gen, 
  steps_per_epoch = 500,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)
plot(history)
```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```