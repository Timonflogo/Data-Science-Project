---
title: "jena_climate_LSTM"
author: "Timon Florian Godt"
date: "5/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```


```{r}
# import libraries
pacman::p_load(dplyr, tibble, keras, readr, ggplot2, tidyverse)
```

```{r}
# download file
dir.create("~/Downloads/jena_climate", recursive = TRUE)
download.file(
  "https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip",
  "~/Downloads/jena_climate/jena_climate_2009_2016.csv.zip"
)
unzip(
  "~/Downloads/jena_climate/jena_climate_2009_2016.csv.zip",
  exdir = "~/Downloads/jena_climate"
)

```

```{r}
data_dir <- "~/Downloads/jena_climate"
fname <- file.path(data_dir, "jena_climate_2009_2016.csv")
data <- read_csv(fname)
```

```{r}
glimpse(data)
```

```{r}
ggplot(data, aes(x = 1:nrow(data), y = data$`T (degC)`)) + geom_line()
```

```{r}
# get plot for first 10 days of temperature
ggplot(data[1:1440,], aes(x = 1:1440, y = `T (degC)`)) + geom_line()
```

```{r}
# we need to preprocess the data so that the RNN can understand it. It is already numerical so we don't have to do vectorization. Each time series is on a different scale which means we will need to normalize each of them independently. Afterwards we are going to write a generator function that takes the current array of float data and yields batches of of data from the recent past, along with a target temperature in the future. Since the samples in the Dataset are highly redundant (sample N and sample N+1 will have most of their timesteps in common as there is only a 10 minute time period between them) it would be wasteful to allocate every sample. we are instead going to generate the samples on the fly using the original data. 
# lets convert our dataframe into a matrix of floating points. we are NOT going to include the DATETIME column. 
data <- as.matrix(data[,-1])
```

```{r}
# to normalize the data we are going to subtract the mean and divide by the standard deviation. We are going to carry out these operations for every time series we have in our matrix. 
# We are going to use the first 200.000 datapoints as our train set and therefore apply our normalization on this subset first.
train_data <- data[1:200000,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
data < - scale(data, center = mean, scale = std)
```

```{r}
# build the generator to yield a list (samples, targets). where samples is one batch of input data and targets is the corresponding array of target temperatures. it takes the following arguments: 
# data - the origin of floating-point data, which we normalized in the previous step
# lookback - How many timesteps back the input data should go
# delay - How many timesteps into the future the target should be
# min_index and max_index - indices in the data array that delimit which timesteps to draw from. this is useful for keeping a segment of the data for validation and another for testing
# shuffle - Whether to shuffle the samples or draw them in chronological order.
# batch_size - The number of samlpes per batch
# step - The period, in timesteps, at which you sample data. You'll set it to 6 in order to get a data point every hour. 

generator <- function(data, lookback, delay, min_index, max_index, shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index <- nrwo(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else { 
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(lenght(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targests <- array(0, dim = c(length(rows)))
    
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]],
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay, 2] 
    }
    
    list(samples, targets)
  }
}
```
I am not entirely sure how this function works but will try to explain later.

```{r}
# lets use the generator function to generate our train, validation, and test datasets
lookback <- 1440 # the amount of timesteps back the input data should go
step <- 6 # the amount of steps in one hour. setting this to 6 will yield us with 1 data point every hour. 
delay <- 144
batch_size <- 128
train_gen <- generator(
  data, 
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 200000,
  shuffle = TRUE,
  step = step,
  batch_size = batch_size
)
val_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 200001,
  max_index = 300000,
  step = step,
  batch_size = batch_size
)
test_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 300001,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)
# this is how many steps to draw from 'val_gen'
# in order to see the whole validation set:
val_steps <- (300000 - 200001 - lookback) / batch_size
# this is how many steps to draw from test_gen
test_steps <- (nrow(data) - 300001 - lookback) / batch_size
```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```